{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow library, which is essential for building and training deep learning models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Keras layers and models modules from TensorFlow for building neural networks\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Import ImageDataGenerator class for real-time image augmentation and data preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import the ResNet50 model from TensorFlow's applications module for transfer learning\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Import Matplotlib's pyplot for plotting graphs and visualizations (e.g., training curves)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import classification metrics from scikit-learn to evaluate the model's performance\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Import NumPy for numerical operations like array manipulation\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for training and validation data\n",
    "train_data_dir = 'data/train'  # Directory for training images\n",
    "validation_data_dir = 'data/validation'  # Directory for validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to store label information for hand gestures used in the sign language detection model.\n",
    "# Each dictionary contains a 'name' representing the gesture and an 'id' that serves as a unique identifier for that gesture.\n",
    "label_info = [\n",
    "    {'name': 'hello', 'id': 1},       # 'hello' gesture mapped to ID 1\n",
    "    {'name': 'thanks', 'id': 2},      # 'thanks' gesture mapped to ID 2\n",
    "    {'name': 'yes', 'id': 3},         # 'yes' gesture mapped to ID 3\n",
    "    {'name': 'no', 'id': 4},          # 'no' gesture mapped to ID 4\n",
    "    {'name': 'iloveyou', 'id': 5}     # 'I love you' gesture mapped to ID 5\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps class indices to corresponding label names.\n",
    "# This is done using a dictionary comprehension that enumerates over the label_info list.\n",
    "label_dict = {i: label['name'] for i, label in enumerate(label_info)}\n",
    "\n",
    "# Print the resulting label dictionary to verify the mapping.\n",
    "print(\"Label dictionary:\", label_dict)  # Output the label dictionary to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the height and width of the images to be processed.\n",
    "# These dimensions should be adjusted based on the requirements of your model \n",
    "# and the characteristics of your dataset. All images will be resized to this size.\n",
    "img_height, img_width = 150, 150  # Image dimensions (height, width)\n",
    "\n",
    "# Set the batch size for processing images during training and validation.\n",
    "# The batch size determines how many images will be fed into the model at once \n",
    "# for training or evaluation, impacting the speed and memory usage during training.\n",
    "batch_size = 8  # Number of images to process in a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced ImageDataGenerator for the training dataset with tuned augmentation parameters.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,               # Normalize pixel values for improved model training stability.\n",
    "    rotation_range=15,                 # Slightly reduce rotation range for a more realistic augmentation.\n",
    "    width_shift_range=0.15,            # Limit horizontal shifts to 15% to avoid excessive transformations.\n",
    "    height_shift_range=0.15,           # Limit vertical shifts to 15%.\n",
    "    shear_range=0.1,                   # Reduce shear transformation to prevent image distortion.\n",
    "    zoom_range=[0.9, 1.1],             # Slight zoom in/out to generalize scale invariance.\n",
    "    horizontal_flip=True,              # Enable horizontal flips (if applicable to your dataset).\n",
    "    brightness_range=[0.8, 1.2],       # Randomly adjust brightness for lighting variations.\n",
    "    fill_mode='nearest'                # Fill in new pixels with nearest values to avoid artifacts.\n",
    ")\n",
    "\n",
    "# Validation ImageDataGenerator with only rescaling for more reliable validation performance.\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator for the training set using the ImageDataGenerator's flow_from_directory method\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,  # Directory containing the training data\n",
    "    target_size=(img_height, img_width),  # Resize the images to the target size (img_height, img_width)\n",
    "    batch_size=batch_size,  # Number of images to process in one batch\n",
    "    class_mode='categorical',  # For multi-class classification (one-hot encoded labels)\n",
    ")\n",
    "\n",
    "# Create a data generator for the validation set, similarly to the training set\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,  # Directory containing the validation data\n",
    "    target_size=(img_height, img_width),  # Resize the images to the target size\n",
    "    batch_size=batch_size,  # Number of images to process in one batch\n",
    "    class_mode='categorical',  # For multi-class classification (one-hot encoded labels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.class_indices)\n",
    "\n",
    "label_dict = {v: k for k, v in train_generator.class_indices.items()}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet50 model with custom input size and no top layer\n",
    "resnet_base = ResNet50(\n",
    "    weights='imagenet',            # Load pre-trained weights from ImageNet\n",
    "    include_top=False,             # Exclude the fully connected layer at the top\n",
    "    input_shape=(img_height, img_width, 3)  # Input image dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the ResNet base model layers\n",
    "resnet_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet_base.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# First fully connected layer with Batch Normalization and L2 regularization\n",
    "x = layers.Dense(516, activation=None, kernel_regularizer='l2')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Higher dropout rate for better regularization\n",
    "\n",
    "# # Second fully connected layer with Batch Normalization and L2 regularization\n",
    "# x = layers.Dense(516, activation=None, kernel_regularizer='l2')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.ReLU()(x)\n",
    "\n",
    "# Third fully connected layer with Batch Normalization and L2 regularization\n",
    "x = layers.Dense(256, activation=None, kernel_regularizer='l2')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Higher dropout rate for better regularization\n",
    "\n",
    "# # Fourth fully connected layer with Batch Normalization and L2 regularization\n",
    "# x = layers.Dense(128, activation=None, kernel_regularizer='l2')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.ReLU()(x)\n",
    "\n",
    "output_layer = layers.Dense(train_generator.num_classes, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = models.Model(inputs=resnet_base.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),  # Use the Adam optimizer for training, which is efficient and widely used.\n",
    "              loss='categorical_crossentropy',  # Specify categorical crossentropy as the loss function for multi-class classification.\n",
    "              metrics=['accuracy'])  # Track accuracy as the performance metric during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Learning Rate Reduction Callback: Reduces learning rate when validation loss has stopped improving.\n",
    "# Monitors 'val_loss' and reduces the learning rate by a smaller factor if no improvement for a specific number of epochs.\n",
    "# A minimum learning rate and cooldown period are included to prevent over-reduction and frequent adjustments.\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',      # Metric to monitor\n",
    "    factor=0.7,              # Reduce the learning rate by half (new_lr = lr * factor)\n",
    "    patience=2,              # Reduce the learning rate after 4 epochs of no improvement\n",
    "    verbose=1,               # Print a message when reducing the learning rate\n",
    "    min_lr=1e-4,             # Minimum learning rate to avoid becoming too low\n",
    "    cooldown=2               # Wait 2 epochs after a reduction before resuming monitoring\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to stop training at 92% accuracy\n",
    "class StopAtAccuracy(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):  # This method is called at the end of each epoch during training.\n",
    "        if logs[\"accuracy\"] >= 0.90:  # Check if the accuracy for the epoch has reached or exceeded 98%.\n",
    "            self.model.stop_training = True  # Stop the training process if the condition is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Train the model using the fit method, which adjusts the model's weights based on training data\n",
    "history = model.fit(\n",
    "    train_generator,  # The training data generator providing batches of training images and labels\n",
    "    validation_data=validation_generator,  # The validation data generator for evaluating the model during training\n",
    "    epochs=epochs,  # The number of complete passes through the training dataset\n",
    "    callbacks=[StopAtAccuracy(), lr_reduction]  # Include callbacks for stopping the training early and reducing the learning rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting.\n",
    "\n",
    "# Create a figure with a specified size for the plots.\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Create the first subplot for accuracy values.\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')  # Plot training accuracy.\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Plot validation accuracy.\n",
    "plt.title('Model Accuracy')  # Title of the accuracy plot.\n",
    "plt.ylabel('Accuracy')  # Label for the y-axis.\n",
    "plt.xlabel('Epoch')  # Label for the x-axis.\n",
    "plt.legend(loc='upper left')  # Add a legend to the plot.\n",
    "\n",
    "# Create the second subplot for loss values.\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.plot(history.history['loss'], label='Train Loss')  # Plot training loss.\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')  # Plot validation loss.\n",
    "plt.title('Model Loss')  # Title of the loss plot.\n",
    "plt.ylabel('Loss')  # Label for the y-axis.\n",
    "plt.xlabel('Epoch')  # Label for the x-axis.\n",
    "plt.legend(loc='upper left')  # Add a legend to the plot.\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap of subplots.\n",
    "plt.show()  # Display the plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training in the Keras format\n",
    "model.save('sign_language_detection_model.keras')  # Save the entire model architecture, weights, and training configuration to a file named 'sign_language_detection_model.keras'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the validation set\n",
    "val_predictions = model.predict(validation_generator)\n",
    "y_pred = np.argmax(val_predictions, axis=1)\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Calculate and display metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=list(label_dict.values())))\n",
    "\n",
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=list(label_dict.values()), cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
