{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TensorFlow library, which provides a wide range of tools for machine learning and deep learning tasks.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import specific modules from Keras, which is a high-level neural networks API \n",
    "# integrated within TensorFlow for building and training deep learning models.\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Import the ImageDataGenerator class from Keras, which is used for real-time data augmentation \n",
    "# and loading images from directories, helping to preprocess images for training and validation.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for training and validation data\n",
    "train_data_dir = 'data/train'  # Directory for training images\n",
    "validation_data_dir = 'data/validation'  # Directory for validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to store label information for hand gestures used in the sign language detection model.\n",
    "# Each dictionary contains a 'name' representing the gesture and an 'id' that serves as a unique identifier for that gesture.\n",
    "label_info = [\n",
    "    {'name': 'hello', 'id': 1},       # 'hello' gesture mapped to ID 1\n",
    "    {'name': 'thanks', 'id': 2},      # 'thanks' gesture mapped to ID 2\n",
    "    {'name': 'yes', 'id': 3},         # 'yes' gesture mapped to ID 3\n",
    "    {'name': 'no', 'id': 4},          # 'no' gesture mapped to ID 4\n",
    "    {'name': 'iloveyou', 'id': 5}     # 'I love you' gesture mapped to ID 5\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps class indices to corresponding label names.\n",
    "# This is done using a dictionary comprehension that enumerates over the label_info list.\n",
    "label_dict = {i: label['name'] for i, label in enumerate(label_info)}\n",
    "\n",
    "# Print the resulting label dictionary to verify the mapping.\n",
    "print(\"Label dictionary:\", label_dict)  # Output the label dictionary to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the height and width of the images to be processed.\n",
    "# These dimensions should be adjusted based on the requirements of your model \n",
    "# and the characteristics of your dataset. All images will be resized to this size.\n",
    "img_height, img_width = 150, 150  # Image dimensions (height, width)\n",
    "\n",
    "# Set the batch size for processing images during training and validation.\n",
    "# The batch size determines how many images will be fed into the model at once \n",
    "# for training or evaluation, impacting the speed and memory usage during training.\n",
    "batch_size = 4  # Number of images to process in a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator for the training dataset to apply real-time data augmentation.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,               # Rescale pixel values to the range [0, 1] for normalization.\n",
    "    rotation_range=20,               # Randomly rotate images in the range of 0 to 20 degrees.\n",
    "    width_shift_range=0.2,           # Randomly shift images horizontally by up to 20% of the image width.\n",
    "    height_shift_range=0.2,          # Randomly shift images vertically by up to 20% of the image height.\n",
    "    shear_range=0.2,                 # Apply shear transformations randomly to the images.\n",
    "    zoom_range=0.2,                  # Randomly zoom in on images by up to 20%.\n",
    "    horizontal_flip=True,             # Randomly flip images horizontally.\n",
    "    fill_mode='nearest'               # Fill in new pixels created during transformations with the nearest pixel values.\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for the validation dataset.\n",
    "# For validation, only rescaling is performed to maintain the integrity of the validation dataset.\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Normalize validation images to the range [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data using the flow_from_directory method of the ImageDataGenerator.\n",
    "# This method generates batches of tensor image data from the training data directory.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,                     # Directory containing the training data organized into subdirectories for each class.\n",
    "    target_size=(img_height, img_width),  # Resize all images to the specified target size (img_height, img_width).\n",
    "    batch_size=batch_size,               # Number of images to yield per batch.\n",
    "    class_mode='categorical'             # Specifies the type of label arrays that are returned: 'categorical' means one-hot encoded labels.\n",
    ")\n",
    "\n",
    "# Load validation data using the flow_from_directory method of the ImageDataGenerator.\n",
    "# Similar to the training data generator, it generates batches of tensor image data from the validation data directory.\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,                # Directory containing the validation data organized into subdirectories for each class.\n",
    "    target_size=(img_height, img_width),  # Resize all images to the specified target size (img_height, img_width).\n",
    "    batch_size=batch_size,               # Number of images to yield per batch.\n",
    "    class_mode='categorical'             # Specifies the type of label arrays that are returned: 'categorical' means one-hot encoded labels.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "# Input layer: Specifies the shape of the input images\n",
    "input_layer = layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# First Convolutional Layer: Applies 32 filters of size 3x3 and uses ReLU activation\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "\n",
    "# First Max Pooling Layer: Reduces the spatial dimensions of the feature maps by a factor of 2\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Batch Normalization: Normalizes the output of the previous layer to improve training speed and stability\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Second Convolutional Layer: Applies 64 filters of size 3x3 and uses ReLU activation\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Second Max Pooling Layer: Again reduces the spatial dimensions of the feature maps\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Batch Normalization: Normalizes the output of the previous layer\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Third Convolutional Layer: Applies 128 filters of size 3x3 and uses ReLU activation\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Third Max Pooling Layer: Reduces the spatial dimensions of the feature maps\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Batch Normalization: Normalizes the output of the previous layer\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Flatten Layer: Flattens the 3D output of the previous layer into a 1D vector\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Fully Connected Layer: A dense layer with 128 neurons and ReLU activation\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Dropout Layer: Randomly sets 50% of the input units to 0 to prevent overfitting during training\n",
    "x = layers.Dropout(0.5)(x) \n",
    "\n",
    "# Output Layer: A dense layer with a number of units equal to the number of classes in the dataset\n",
    "# It uses softmax activation to output class probabilities\n",
    "output_layer = layers.Dense(train_generator.num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)  # Instantiate the model with the specified input and output layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # Use the Adam optimizer for training, which is efficient and widely used.\n",
    "              loss='categorical_crossentropy',  # Specify categorical crossentropy as the loss function for multi-class classification.\n",
    "              metrics=['accuracy'])  # Track accuracy as the performance metric during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to stop training at 92% accuracy\n",
    "class StopAtAccuracy(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):  # This method is called at the end of each epoch during training.\n",
    "        if logs[\"accuracy\"] >= 0.92:  # Check if the accuracy for the epoch has reached or exceeded 92%.\n",
    "            self.model.stop_training = True  # Stop the training process if the condition is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Learning Rate Reduction Callback: This callback reduces the learning rate when a metric has stopped improving.\n",
    "# It monitors the validation loss ('val_loss') during training.\n",
    "# If the validation loss does not improve for a specified number of epochs (patience), \n",
    "# the learning rate will be reduced by a factor (factor).\n",
    "# The 'verbose' parameter, when set to 1, will print a message when the learning rate is reduced.\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # The metric to monitor for improvement\n",
    "    factor=0.2,          # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1            # If set to 1, prints a message when the learning rate is reduced\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Train the model using the fit method, which adjusts the model's weights based on training data\n",
    "history = model.fit(\n",
    "    train_generator,  # The training data generator providing batches of training images and labels\n",
    "    validation_data=validation_generator,  # The validation data generator for evaluating the model during training\n",
    "    epochs=epochs,  # The number of complete passes through the training dataset\n",
    "    callbacks=[StopAtAccuracy(), lr_reduction]  # Include callbacks for stopping the training early and reducing the learning rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting.\n",
    "\n",
    "# Create a figure with a specified size for the plots.\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Create the first subplot for accuracy values.\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')  # Plot training accuracy.\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Plot validation accuracy.\n",
    "plt.title('Model Accuracy')  # Title of the accuracy plot.\n",
    "plt.ylabel('Accuracy')  # Label for the y-axis.\n",
    "plt.xlabel('Epoch')  # Label for the x-axis.\n",
    "plt.legend(loc='upper left')  # Add a legend to the plot.\n",
    "\n",
    "# Create the second subplot for loss values.\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.plot(history.history['loss'], label='Train Loss')  # Plot training loss.\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')  # Plot validation loss.\n",
    "plt.title('Model Loss')  # Title of the loss plot.\n",
    "plt.ylabel('Loss')  # Label for the y-axis.\n",
    "plt.xlabel('Epoch')  # Label for the x-axis.\n",
    "plt.legend(loc='upper left')  # Add a legend to the plot.\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap of subplots.\n",
    "plt.show()  # Display the plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training in the Keras format\n",
    "model.save('sign_language_detection_model.keras')  # Save the entire model architecture, weights, and training configuration to a file named 'sign_language_detection_model.keras'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
